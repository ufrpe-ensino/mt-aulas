{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ufrpe-ensino/mt-aulas/blob/master/03a_Classifica%C3%A7%C3%A3oTextos_BoW_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x9LVwe-LiMi"
      },
      "source": [
        "# Aprendizado Bayesiano: Naive Bayes\n",
        "\n",
        "O algoritmo Naive Bayes, assim como o KNN é um algoritmo de aprendizado com implementação relativamente simples, e que pode levar a resultados muito bons em determinados problemas de classificação. Este consiste em aplicar o teorema de Bayes, com a prerrogativa de independencia condicional entre cada par de atributos dado o valor da classe.\n",
        "\n",
        "Clique nos links para mais informações sobre o algoritmo [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) e sobre [classificação de textos](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). com scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMvflrmwLnJl"
      },
      "source": [
        "# Exemplo: classificação de texto\n",
        "\n",
        "Neste exemplo, utilizaremos um dataset chamado 20newsgroups.\n",
        "\n",
        "O conjunto de dados é uma coleção de aproximadamente 20.000 documentos de grupos de notícias, particionados (quase) uniformemente em 20 grupos/categorias de notícias diferentes. Mais informações sobre esta base podem ser obtidas no repositório [UCI](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)\n",
        "\n",
        "As categorias existentes são:\n",
        "\n",
        "* 'alt.atheism',\n",
        "* 'comp.graphics',\n",
        "* 'comp.os.ms-windows.misc',\n",
        "* 'comp.sys.ibm.pc.hardware',\n",
        "* 'comp.sys.mac.hardware',\n",
        "* 'comp.windows.x',\n",
        "* 'misc.forsale',\n",
        "* 'rec.autos',\n",
        "* 'rec.motorcycles',\n",
        "* 'rec.sport.baseball',\n",
        "* 'rec.sport.hockey',\n",
        "* 'sci.crypt',\n",
        "* 'sci.electronics',\n",
        "* 'sci.med',\n",
        "* 'sci.space',\n",
        "* 'soc.religion.christian',\n",
        "* 'talk.politics.guns',\n",
        "* 'talk.politics.mideast',\n",
        "* 'talk.politics.misc',\n",
        "* 'talk.religion.misc'\n",
        "\n",
        "Neste exemplo, vamos considerar apenas duas categorias: '**alt.atheism**' e '**comp.graphics**'. O scitkit contém uma função que auxilia o download desta base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeOVilr9NIUG",
        "outputId": "39cc56a8-07ed-4946-95f1-0292ec2d4c54"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Categorias selecionadas\n",
        "categories = [\n",
        "    'alt.atheism',\n",
        "    'comp.graphics',\n",
        "]\n",
        "\n",
        "print(\"Carregando 20 newsgroups dataset para as categorias:\")\n",
        "print(categories)\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)\n",
        "print(\"%d documents\" % len(twenty_train.filenames))\n",
        "print(\"%d categories\" % len(twenty_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4g3K1-UlcW",
        "outputId": "1bd8c537-f542-42fe-9315-b5843b5021b9"
      },
      "outputs": [],
      "source": [
        "# Visualizar os dados coletados\n",
        "print(twenty_train.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC-a8vReVU70",
        "outputId": "6273b7fa-adcb-4d14-c962-90210308bbc0"
      },
      "outputs": [],
      "source": [
        "# Visualizando um dos documentos\n",
        "print(twenty_train['data'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqq08LkczeD",
        "outputId": "af1f3ec4-c2d6-4b43-e762-3bf50448ec8a"
      },
      "outputs": [],
      "source": [
        "# tamanho (sem preprocessamento)\n",
        "len(twenty_train['data'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1UM10zQPV_k"
      },
      "source": [
        "## Preprocessamento\n",
        "\n",
        "Como etapa de préprocessamento, iremos apenas nos limitar a remover palavras muito comuns, as chamadas *stopwords*. Uma lista de possíveis stopwords para inglês encontra-se abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOtv0Xd7Pj8m"
      },
      "outputs": [],
      "source": [
        "stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        "             'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',  'can', \"can't\", 'cannot', 'could', \"couldn't\",\n",
        "             'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during','each', 'few', 'for', 'from', 'further',\n",
        "             'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\",\n",
        "             'it', \"it's\", 'its', 'itself','1st', '2nd', '3rd','4th', '5th', '6th', '7th', '8th', '9th', '10th'\n",
        "             \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself','no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        "             'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such',\n",
        "             'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\",\n",
        "             \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very',\n",
        "             'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
        "             \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\",\n",
        "             'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves',\n",
        "             'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ghOFTgKtzh"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Complemente o código abaixo para:\n",
        "* descartar stopwords\n",
        "* descartar palavras com menos de 3 caracteres\n",
        "* descartar tokens contendo apenas numeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRtr11vNRMof",
        "outputId": "8250ad7b-2c7f-497b-c028-7495caeac989"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "  # remover pontuações\n",
        "  text   = text.translate(string.punctuation)\n",
        "\n",
        "  # converter para lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # tokenizar o texto em palavras\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "  # SEU CÓDIGO AQUI\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "X_train = []\n",
        "for doc in twenty_train['data']:\n",
        "  X_train.append(preprocess(doc))\n",
        "\n",
        "X_test = []\n",
        "for doc in twenty_test['data']:\n",
        "  X_test.append(preprocess(doc))\n",
        "\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxcZX12qc5Sr",
        "outputId": "44f60e5f-ed97-4c9f-cd2f-8ab80c37011d"
      },
      "outputs": [],
      "source": [
        "# tamanho (com preprocessamento)\n",
        "len(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwUHXRvfMOc-"
      },
      "source": [
        "## Representação vetorial do texto\n",
        "\n",
        "Neste exemplo, utilizaremos o algoritmo Naive Bayes para classificar documentos de texto em categorias. Para isso, precisamos antes converter o texto para uma representação vetorial, ou seja, cada documento/exemplo precisa ser representado por um vetor de dimensões pré-definidas. Utilizaremos três técnicas básicas: BOW (*Bag of Words*), TF (*Term Frequency*) e TF-IDF (*Term Frequency - Inverse Document Frequency*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6o5yjjaPJCG"
      },
      "source": [
        "### BOW\n",
        "\n",
        "Consiste basicamente em contar quantas vezes cada palavra aparece no documento. Ou seja, sua aplicação a um conjunto de *n* documentos, produz uma matriz *n x d*, onde *d* corresponde ao tamanho do vocabulário considerado. No Scikit, esta representação é implementada pelo [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMmOZfv2SsZ4",
        "outputId": "e26c692a-82f2-47c6-a034-74705451b75d"
      },
      "outputs": [],
      "source": [
        "# Exemplo de uso do BOW no scikit\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtimrxTfTs8T"
      },
      "source": [
        "Aplicando ao nosso dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkcoN3_XTu-I",
        "outputId": "8228f70c-4fa8-410b-d50a-5610becd31ca"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_model  = vectorizer.fit(X_train)\n",
        "\n",
        "X_bow_train = bow_model.transform(X_train)\n",
        "X_bow_test  = bow_model.transform(X_test)\n",
        "\n",
        "print(X_bow_train.shape,X_bow_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB-lp5d-Wej9",
        "outputId": "6ee0f80b-d176-4229-df2c-267809de246d"
      },
      "outputs": [],
      "source": [
        "# matriz está armazenada em formato sparse\n",
        "print(X_bow_train[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6COLPTEVpJm"
      },
      "source": [
        "### *Term Frequency* (TF)\n",
        "A contagem de ocorrências (i.e., BOW) é um bom começo, mas há um problema: documentos mais longos terão valores de contagem média mais altos do que documentos mais curtos, embora possam falar sobre os mesmos tópicos.\n",
        "\n",
        "Para evitar essas possíveis discrepâncias, basta dividir o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbJtPIvSVtke",
        "outputId": "6ac8b4c7-a8e2-4d02-ef06-2665d24a59cf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=False)\n",
        "tf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tf_train = tf_model.transform(X_train)\n",
        "X_tf_test  = tf_model.transform(X_test)\n",
        "\n",
        "print(X_tf_train[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjQ0ITVjX8fV"
      },
      "source": [
        "### TF-IDF\n",
        "Usando o scikit, basta ativar o flag `use_idf=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUoJsm_YFG3",
        "outputId": "116dfef6-8d90-492c-b3b2-91fd1627c633"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "tfidf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tfidf_train = tfidf_model.transform(X_train)\n",
        "X_tfidf_test  = tfidf_model.transform(X_test)\n",
        "\n",
        "print(X_tfidf_train[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTJq9ya-Yw0z"
      },
      "source": [
        "# Treinando modelo NaiveBayes (NB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYV00SwlbDhF"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW8Q3aN-axP_"
      },
      "source": [
        "## BOW + NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDOh6kBpbAN5",
        "outputId": "2ba5f66e-71d1-486f-d2bf-57fe63d0df50"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_bow_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_bow_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PFI2MQ-a0_4"
      },
      "source": [
        "## TF + NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QMVfd0buzP",
        "outputId": "6c2d9025-e744-40a0-9c2f-5ba47eac1218"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_tf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgfiTAx8a2zk"
      },
      "source": [
        "## TF-IDF + NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y24I7yk0Y2Wh",
        "outputId": "60d0f982-da74-4ad6-8e58-5648ff50dc36"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_tfidf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tfidf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercício: explore outras métricas\n",
        "\n",
        "Utilize a função `sklearn.metrics.ConfusionMatrixDisplay` para visualizar a matriz de confusão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEU CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilize a função `sklearn.metrics.classification_report` para visualizar a as métricas de precision, recall e F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEU CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crie um dataframe pandas, e visualize os casos em que os modelos erraram de cada classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEU CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzxDimJCaN-x"
      },
      "source": [
        "# Testando iterativamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BictP9Z9Z35L",
        "outputId": "a3547967-8593-4146-c5ad-631d716ecf3e"
      },
      "outputs": [],
      "source": [
        "docs = ['God is love', 'OpenGL on the GPU is fast']\n",
        "preprocessed_docs = [preprocess(doc) for doc in docs]\n",
        "\n",
        "print('\\nafter preprocessing:')\n",
        "print(preprocessed_docs)\n",
        "\n",
        "docs_preds = clf.predict(tfidf_model.transform(preprocessed_docs))\n",
        "\n",
        "print('\\npredictions:')\n",
        "for i,doc in enumerate(docs):\n",
        "    print('{} -> {}'.format(doc, twenty_train.target_names[docs_preds[i]]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = input('Digite um texto: ')\n",
        "\n",
        "preprocessed_text = preprocess(text)\n",
        "print('\\nafter preprocessing:')\n",
        "print(preprocessed_text)\n",
        "docs_preds = clf.predict(tfidf_model.transform([preprocessed_text]))\n",
        "print('\\npredictions:')\n",
        "for i,doc in enumerate([text]):\n",
        "    print('{} -> {}'.format(doc, twenty_train.target_names[docs_preds[i]]))\n",
        "print('\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "03_ClassificaçãoTextos-BoW-TF-IDF.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
